'use strict';

var _slicedToArray = function () { function sliceIterator(arr, i) { var _arr = []; var _n = true; var _d = false; var _e = undefined; try { for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i["return"]) _i["return"](); } finally { if (_d) throw _e; } } return _arr; } return function (arr, i) { if (Array.isArray(arr)) { return arr; } else if (Symbol.iterator in Object(arr)) { return sliceIterator(arr, i); } else { throw new TypeError("Invalid attempt to destructure non-iterable instance"); } }; }();

var uploadAssets = function () {
  var _ref = _asyncToGenerator(function* (assets, options) {
    // Build a Map where the keys are `type#url` and the value is an array of all
    // objects containing document id and path to inject asset reference to.
    // `assets` is an array of objects with shape: {documentId, path, url, type}
    var assetMap = getAssetMap(assets);

    // Create a function we can call for every completed upload to report progress
    var progress = progressStepper(options.onProgress, {
      step: 'Importing assets (files/images)',
      total: assetMap.size
    });

    // Loop over all unique URLs and ensure they exist, and if not, upload them
    var mapOptions = { concurrency: ASSET_UPLOAD_CONCURRENCY };
    var assetIds = yield pMap(assetMap.keys(), ensureAsset.bind(null, options, progress), mapOptions);

    // Loop over all documents that need asset references to be set
    var batches = yield setAssetReferences(assetMap, assetIds, options);
    return batches.reduce(function (prev, add) {
      return prev + add;
    }, 0);
  });

  return function uploadAssets(_x, _x2) {
    return _ref.apply(this, arguments);
  };
}();

var ensureAsset = function () {
  var _ref2 = _asyncToGenerator(function* (options, progress, assetKey, i) {
    var client = options.client;

    var _assetKey$split = assetKey.split('#', 2),
        _assetKey$split2 = _slicedToArray(_assetKey$split, 2),
        type = _assetKey$split2[0],
        url = _assetKey$split2[1];

    // Download the asset in order for us to create a hash


    debug('[Asset #%d] Downloading %s', i, url);
    var buffer = yield getBufferForUri(url);
    var label = getHash(buffer);

    // See if the item exists on the server
    debug('[Asset #%d] Checking for asset with hash %s', i, label);
    var assetId = yield getAssetIdForLabel(client, type, label);
    if (assetId) {
      // Same hash means we want to reuse the asset
      debug('[Asset #%d] Found %s for hash %s', i, type, label);
      progress();
      return assetId;
    }

    var _parseUrl = parseUrl(url),
        pathname = _parseUrl.pathname;

    var filename = basename(pathname);

    // If it doesn't exist, we want to upload it
    debug('[Asset #%d] Uploading %s with URL %s', i, type, url);
    var asset = yield client.assets.upload(type, buffer, { label, filename });
    progress();
    return asset._id;
  });

  return function ensureAsset(_x3, _x4, _x5, _x6) {
    return _ref2.apply(this, arguments);
  };
}();

var getAssetIdForLabel = function () {
  var _ref3 = _asyncToGenerator(function* (client, type, label) {
    var attemptNum = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;

    // @todo remove retry logic when client has reintroduced it
    try {
      var dataType = type === 'file' ? 'sanity.fileAsset' : 'sanity.imageAsset';
      var query = '*[_type == $dataType && label == $label][0]._id';
      var assetDocId = yield client.fetch(query, { dataType, label });
      return assetDocId;
    } catch (err) {
      if (attemptNum < 3) {
        return getAssetIdForLabel(client, type, label, attemptNum + 1);
      }

      err.attempts = attemptNum;
      throw new Error(`Error while attempt to query Sanity API:\n${err.message}`);
    }
  });

  return function getAssetIdForLabel(_x8, _x9, _x10) {
    return _ref3.apply(this, arguments);
  };
}();

function _asyncToGenerator(fn) { return function () { var gen = fn.apply(this, arguments); return new Promise(function (resolve, reject) { function step(key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { return Promise.resolve(value).then(function (value) { step("next", value); }, function (err) { step("throw", err); }); } } return step("next"); }); }; }

var basename = require('path').basename;
var parseUrl = require('url').parse;
var debug = require('debug')('sanity:import');
var crypto = require('crypto');
var pMap = require('p-map');
var getBufferForUri = require('./util/getBufferForUri');
var progressStepper = require('./util/progressStepper');

var ASSET_UPLOAD_CONCURRENCY = 3;
var ASSET_PATCH_CONCURRENCY = 3;
var ASSET_PATCH_BATCH_SIZE = 50;

function getAssetMap(assets) {
  return assets.reduce(function (assetMap, item) {
    var documentId = item.documentId,
        path = item.path,
        url = item.url,
        type = item.type;

    var key = `${type}#${url}`;
    var refs = assetMap.get(key);
    if (!refs) {
      refs = [];
      assetMap.set(key, refs);
    }

    refs.push({ documentId, path });
    return assetMap;
  }, new Map());
}

function getHash(buffer) {
  return crypto.createHash('sha256').update(buffer).digest('hex');
}

function setAssetReferences(assetMap, assetIds, options) {
  var client = options.client;

  var lookup = assetMap.values();
  var patchTasks = assetIds.reduce(function (tasks, assetId) {
    var documents = lookup.next().value;
    return tasks.concat(documents.map(function (_ref4) {
      var documentId = _ref4.documentId,
          path = _ref4.path;
      return {
        documentId,
        path,
        assetId
      };
    }));
  }, []);

  // We now have an array of simple tasks, each containing:
  // {documentId, path, assetId}
  // Instead of doing a single mutation per asset, let's batch them up
  var batches = [];
  for (var i = 0; i < patchTasks.length; i += ASSET_PATCH_BATCH_SIZE) {
    batches.push(patchTasks.slice(i, i + ASSET_PATCH_BATCH_SIZE));
  }

  // Since separate progress step for batches of reference sets
  var progress = progressStepper(options.onProgress, {
    step: 'Setting asset references to documents',
    total: batches.length
  });

  // Now perform the batch operations in parallel with a given concurrency
  var mapOptions = { concurrency: ASSET_PATCH_CONCURRENCY };
  return pMap(batches, setAssetReferenceBatch.bind(null, client, progress), mapOptions);
}

function setAssetReferenceBatch(client, progress, batch) {
  debug('Setting asset references on %d documents', batch.length);
  return batch.reduce(reducePatch, client.transaction()).commit({ visibility: 'async' }).then(progress).then(function (res) {
    return res.results.length;
  });
}

function getAssetType(assetId) {
  return assetId.slice(0, assetId.indexOf('-'));
}

function reducePatch(trx, task) {
  return trx.patch(task.documentId, function (patch) {
    return patch.set({
      [`${task.path}._type`]: getAssetType(task.assetId),
      [`${task.path}.asset`]: {
        _type: 'reference',
        _ref: task.assetId
      }
    });
  });
}

module.exports = uploadAssets;